<div align="center">

# Mekhanikube ğŸ”§

**Your Kubernetes AI Mechanic**

[![Docker Build](https://github.com/jorgegabrielti/mekhanikube/actions/workflows/docker-build.yml/badge.svg)](https://github.com/jorgegabrielti/mekhanikube/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)](https://github.com/jorgegabrielti/mekhanikube/releases)
[![K8sGPT](https://img.shields.io/badge/K8sGPT-latest-brightgreen.svg)](https://github.com/k8sgpt-ai/k8sgpt)
[![Ollama](https://img.shields.io/badge/Ollama-latest-orange.svg)](https://ollama.ai/)

AnÃ¡lise inteligente de clusters Kubernetes usando K8sGPT com IA local (Ollama). Diagnostica problemas, explica causas e sugere soluÃ§Ãµes automaticamente.

[Quick Start](#-quick-start) â€¢
[Documentation](docs/) â€¢
[Contributing](CONTRIBUTING.md) â€¢
[Changelog](CHANGELOG.md)

</div>

---

## ğŸš€ Quick Start

### Prerequisites

- [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/)
- Active Kubernetes cluster with configured kubeconfig
- At least 8GB of free disk space for AI models

### Installation

```bash
# Clone the repository
git clone https://github.com/jorgegabrielti/mekhanikube.git
cd mekhanikube

# (Optional) Copy and customize environment variables
cp .env.example .env

# Complete setup: build, start services, and install AI model
make setup

# Or step by step:
make build          # Build Docker images
make up             # Start services
make install-model  # Download AI model (gemma:7b ~5GB)
```

### Quick Analysis

```bash
# Analyze your cluster with AI explanations
make analyze

# Or using docker-compose directly:
docker exec mekhanikube-k8sgpt k8sgpt analyze --explain
```

### Makefile Commands

```bash
make help           # Show all available commands
make status         # Check service status
make logs           # View logs
make health         # Run health checks
make analyze-pods   # Analyze only Pods
make test           # Run integration tests
```

## ğŸ“‹ Comandos K8sGPT

```powershell
# Analisar cluster (sem IA)
docker exec mekhanikube-k8sgpt k8sgpt analyze

# Analisar com explicaÃ§Ãµes da IA
docker exec mekhanikube-k8sgpt k8sgpt analyze --explain

# Analisar namespace especÃ­fico
docker exec mekhanikube-k8sgpt k8sgpt analyze -n kube-system --explain

# Filtrar por tipo de recurso
docker exec mekhanikube-k8sgpt k8sgpt analyze --filter=Pod --explain
docker exec mekhanikube-k8sgpt k8sgpt analyze --filter=Service --explain

# Listar filtros disponÃ­veis
docker exec mekhanikube-k8sgpt k8sgpt filters list

# Verificar configuraÃ§Ã£o
docker exec mekhanikube-k8sgpt k8sgpt auth list
```

## ğŸ› ï¸ ConfiguraÃ§Ã£o

### Modelos Ollama Recomendados

```powershell
# Gemma 7B (recomendado - boa qualidade)
docker exec mekhanikube-ollama ollama pull gemma:7b

# Mistral (alternativa)
docker exec mekhanikube-ollama ollama pull mistral

# TinyLlama (mais rÃ¡pido, qualidade inferior)
docker exec mekhanikube-ollama ollama pull tinyllama
```

### Trocar modelo

```powershell
# Remover backend atual
docker exec mekhanikube-k8sgpt k8sgpt auth remove --backend localai

# Adicionar com novo modelo
docker exec mekhanikube-k8sgpt k8sgpt auth add --backend localai --model mistral --baseurl http://localhost:11434/v1
docker exec mekhanikube-k8sgpt k8sgpt auth default -p localai
```

## ğŸ“Š Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kubernetes    â”‚
â”‚     Cluster     â”‚
â”‚   (em VM/Host)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ kubeconfig (montado em /root/.kube/)
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  k8sgpt container â”‚
    â”‚  - Ajusta config  â”‚
    â”‚    automaticamenteâ”‚
    â”‚  - Roda anÃ¡lises  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ API calls (http://localhost:11434/v1)
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ollama container  â”‚
    â”‚  - Gemma:7b model â”‚
    â”‚  - Gera explicaÃ§Ãµesâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Troubleshooting

### K8sGPT nÃ£o consegue acessar cluster

```powershell
# Verificar se kubeconfig estÃ¡ montado
docker exec mekhanikube-k8sgpt ls -la /root/.kube/

# Verificar se config_mod foi criado pelo entrypoint
docker exec mekhanikube-k8sgpt cat /root/.kube/config_mod

# Testar conexÃ£o manual
docker exec mekhanikube-k8sgpt kubectl get nodes
```

### Ollama nÃ£o responde

```powershell
# Ver logs
docker logs mekhanikube-ollama

# Verificar modelos instalados
docker exec mekhanikube-ollama ollama list

# Testar API
Invoke-RestMethod -Uri http://localhost:11434/v1/models | ConvertTo-Json

# Baixar modelo novamente
docker exec mekhanikube-ollama ollama pull gemma:7b
```

### Container k8sgpt nÃ£o inicia

```powershell
# Ver logs
docker logs mekhanikube-k8sgpt

# Reconstruir imagem
docker-compose build k8sgpt
docker-compose up -d k8sgpt
```

## ğŸ“š Recursos

- [K8sGPT Docs](https://docs.k8sgpt.ai/)
- [Ollama Models](https://ollama.com/library)
- [K8sGPT GitHub](https://github.com/k8sgpt-ai/k8sgpt)

## ï¿½ Documentation

- ğŸ“– **[Architecture](docs/ARCHITECTURE.md)** - System design and components
- ğŸ”§ **[Troubleshooting](docs/TROUBLESHOOTING.md)** - Common issues and solutions
- â“ **[FAQ](docs/FAQ.md)** - Frequently asked questions
- ğŸ“‚ **[Project Structure](docs/PROJECT_STRUCTURE.md)** - File organization
- ğŸ¤ **[Contributing](CONTRIBUTING.md)** - How to contribute
- ğŸ”’ **[Security](SECURITY.md)** - Security policy
- ğŸ“ **[Changelog](CHANGELOG.md)** - Version history

## ğŸ” How It Works

1. **Automatic Configuration**: The k8sgpt container runs `/entrypoint.sh` at startup:
   - Copies the mounted kubeconfig from `/root/.kube/config`
   - Replaces `127.0.0.1` with `host.docker.internal` for container networking
   - Saves modified config to `/root/.kube/config_mod`
   - Sets `KUBECONFIG=/root/.kube/config_mod`
   - Configures K8sGPT backend with Ollama

2. **Cluster Analysis**: K8sGPT scans your Kubernetes cluster:
   - Detects issues across Pods, Services, Deployments, etc.
   - Identifies misconfigurations and errors
   - Collects relevant context

3. **AI Explanation**: For each issue found:
   - K8sGPT sends problem context to Ollama
   - LLM generates human-readable explanation
   - Suggests potential solutions

4. **Results**: Clear, actionable output with:
   - Problem description
   - AI-generated explanation
   - Suggested remediation steps

2. **AnÃ¡lise**: K8sGPT escaneia o cluster e identifica problemas (ConfigMaps nÃ£o usados, Pods com erro, etc)

3. **ExplicaÃ§Ã£o**: Quando usa `--explain`, K8sGPT envia o problema para Ollama via API REST

4. **Resposta**: Ollama processa com o modelo gemma:7b e retorna explicaÃ§Ã£o + soluÃ§Ã£o



Se vocÃª jÃ¡ tem Ollama rodando:export OLLAMA_MODEL=mistral



```powershell```2. Inicie o Ollama:

# O programa detecta automaticamente

.\kube-ai.exe```bash

```

## Usodocker-compose up -d

**Nota:** Ollama Ã© significativamente mais lento (1-2 minutos por scan).

```

---

```bash

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

# Iniciar chat interativo3. Instale o modelo Mistral:

### VariÃ¡veis de Ambiente

./kube-ai```bash

```powershell

# ForÃ§ar uso de LocalAIdocker exec -it ollama ollama pull mistral

$env:LLM_PROVIDER="localai"

$env:LOCALAI_URL="http://localhost:8080"# Comandos disponÃ­veis:```

$env:LOCALAI_MODEL="phi-2"

# scan    - Escanear cluster em busca de problemas

# ForÃ§ar uso de Ollama

$env:LLM_PROVIDER="ollama"# exit    - Sair do chat4. Compile e instale a CLI:

$env:OLLAMA_URL="http://localhost:11434"

$env:OLLAMA_MODEL="mistral"# qualquer texto - Fazer perguntas sobre Kubernetes```bash

```

```go install ./cmd/kube-ai

---

```

## ğŸ“Š ComparaÃ§Ã£o de Performance

## Exemplos

| Provider | Modelo    | Tempo/Scan | Qualidade | RAM   |

|----------|-----------|------------|-----------|-------|## Uso

| LocalAI  | phi-2     | ~5-10s     | â­â­â­â­    | 2GB   |

| Ollama   | mistral   | ~60-120s   | â­â­â­â­â­  | 4GB   |```

| Ollama   | tinyllama | ~30-60s    | â­â­â­     | 2GB   |

> scanSimplesmente execute:

**RecomendaÃ§Ã£o:** Use LocalAI com phi-2 para melhor balance entre velocidade e qualidade.

ğŸ” Escaneando cluster...```bash

---

ğŸ¤– Analisando 2 problemas encontrados...kube-ai

## ğŸ› ï¸ Troubleshooting

```

### LocalAI nÃ£o inicia

> O que Ã© um CrashLoopBackOff?

```powershell

# Verifique se o modelo foi baixadoğŸ¤– CrashLoopBackOff indica que um container estÃ¡ falhando...A ferramenta irÃ¡:

dir .\models\

1. Conectar ao seu cluster Kubernetes

# Verifique logs do container

docker-compose logs localai> Como debugar um pod?2. Procurar por pods com problemas



# Reinicie o serviÃ§oğŸ¤– Use kubectl describe pod <name> para ver eventos...3. Coletar informaÃ§Ãµes detalhadas

docker-compose restart

``````4. Usar IA local para analisar e sugerir soluÃ§Ãµes



### Scan muito lento

Se nenhum problema for encontrado, vocÃª verÃ¡:

- âœ… **SoluÃ§Ã£o:** Use LocalAI em vez de Ollama```

- Execute: `.\download-model.ps1` e `docker-compose up -d`âœ… Cluster saudÃ¡vel

```

### Erro de conexÃ£o com Kubernetes

Se problemas forem encontrados, vocÃª receberÃ¡ uma anÃ¡lise detalhada com:

```powershell- Causa provÃ¡vel do problema

# Verifique se o cluster estÃ¡ acessÃ­vel- Como resolver o problema

kubectl cluster-info- Como prevenir que aconteÃ§a novamente

go mod init kube-ai

# Verifique o contexto atualgo get k8s.io/client-go

kubectl config current-contextgo build -o kube-ai ./cmd/kube-ai

``````



---## Uso



## ğŸ“¦ Requisitos```bash

./kube-ai

- **Go:** 1.21 ou superior```

- **Docker Desktop:** Com Kubernetes habilitado

- **RAM:** 4GB disponÃ­vel## Estrutura do Projeto

- **Disco:** 2GB para modelo Phi-2

```

---kube-ai/

 â”œâ”€â”€ cmd/

## ğŸ—ï¸ Arquitetura â”‚    â””â”€â”€ kube-ai/        # main.go, parsing de comandos CLI

 â”œâ”€â”€ internal/

``` â”‚    â”œâ”€â”€ k8s/            # conexÃ£o + scanner

kube-ai/ â”‚    â”‚    â”œâ”€â”€ connect.go

â”œâ”€â”€ cmd/kube-ai/          # CLI principal â”‚    â”‚    â””â”€â”€ scan.go

â”œâ”€â”€ internal/ â”‚    â”œâ”€â”€ llm/            # integraÃ§Ã£o com ollama

â”‚   â”œâ”€â”€ k8s/             # Cliente Kubernetes â”‚    â”‚    â””â”€â”€ ollama.go

â”‚   â””â”€â”€ llm/             # Cliente LLM (LocalAI/Ollama) â”‚    â””â”€â”€ explain/        # heurÃ­sticas e montagem de prompts

â”œâ”€â”€ models/              # Modelos de IA â”‚         â””â”€â”€ explain.go

â”œâ”€â”€ docker-compose.yml   # LocalAI setup â”œâ”€â”€ go.mod

â””â”€â”€ download-model.ps1   # Script para baixar Phi-2 â””â”€â”€ README.md

``````

---

## ğŸ“ LicenÃ§a

MIT


